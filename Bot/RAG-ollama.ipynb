{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf04456",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37109082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/anishbawdhankar/Study/Study/Desktop/RAG_v1/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from langchain_core.documents import Document\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch \n",
    "import numpy as np\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.messages import HumanMessage\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import base64\n",
    "import io\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import ollama_python as ollama\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "747e8579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Clip Model\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "### initialize the Clip Model for unified embeddings\n",
    "clip_model=CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor=CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "255d8983",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Embedding functions\n",
    "def embed_image(image_data):\n",
    "    \"\"\"Embed image using CLIP\"\"\"\n",
    "    if isinstance(image_data, str):  # If path\n",
    "        image = Image.open(image_data).convert(\"RGB\")\n",
    "    else:  # If PIL Image\n",
    "        image = image_data\n",
    "    \n",
    "    inputs=clip_processor(images=image,return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "        # Normalize embeddings to unit vector\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.squeeze().numpy()\n",
    "    \n",
    "def embed_text(text):\n",
    "    \"\"\"Embed text using CLIP.\"\"\"\n",
    "    inputs = clip_processor(\n",
    "        text=text, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=77  # CLIP's max token length\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "        # Normalize embeddings\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c48e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process PDF\n",
    "pdf_path=\"multimodal_sample.pdf\"\n",
    "doc=fitz.open(pdf_path)\n",
    "# Storage for all documents and embeddings\n",
    "all_docs = []\n",
    "all_embeddings = []\n",
    "image_data_store = {}  # Store actual image data for LLM\n",
    "\n",
    "# Text splitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "528cf566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document('multimodal_sample.pdf')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bafd533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,page in enumerate(doc):\n",
    "    ## process text\n",
    "    text=page.get_text()\n",
    "    if text.strip():\n",
    "        ##create temporary document for splitting\n",
    "        temp_doc = Document(page_content=text, metadata={\"page\": i, \"type\": \"text\"})\n",
    "        text_chunks = splitter.split_documents([temp_doc])\n",
    "\n",
    "        #Embed each chunk using CLIP\n",
    "        for chunk in text_chunks:\n",
    "            embedding = embed_text(chunk.page_content)\n",
    "            all_embeddings.append(embedding)\n",
    "            all_docs.append(chunk)\n",
    "\n",
    "\n",
    "\n",
    "    ## process images\n",
    "    ##Three Important Actions:\n",
    "\n",
    "    ##Convert PDF image to PIL format\n",
    "    ##Store as base64 for GPT-4V (which needs base64 images)\n",
    "    ##Create CLIP embedding for retrieval\n",
    "\n",
    "    for img_index, img in enumerate(page.get_images(full=True)):\n",
    "        try:\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            \n",
    "            # Convert to PIL Image\n",
    "            pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "            \n",
    "            # Create unique identifier\n",
    "            image_id = f\"page_{i}_img_{img_index}\"\n",
    "            \n",
    "            # Store image as base64 for later use with GPT-4V\n",
    "            buffered = io.BytesIO()\n",
    "            pil_image.save(buffered, format=\"PNG\")\n",
    "            img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "            image_data_store[image_id] = img_base64\n",
    "            \n",
    "            # Embed image using CLIP\n",
    "            embedding = embed_image(pil_image)\n",
    "            all_embeddings.append(embedding)\n",
    "            \n",
    "            # Create document for image\n",
    "            image_doc = Document(\n",
    "                page_content=f\"[Image: {image_id}]\",\n",
    "                metadata={\"page\": i, \"type\": \"image\", \"image_id\": image_id}\n",
    "            )\n",
    "            all_docs.append(image_doc)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_index} on page {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "doc.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cc2958a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 0, 'type': 'text'}, page_content='Annual Revenue Overview\\nThis document summarizes the revenue trends across Q1, Q2, and Q3. As illustrated in the chart\\nbelow, revenue grew steadily with the highest growth recorded in Q3.\\nQ1 showed a moderate increase in revenue as new product lines were introduced. Q2 outperformed\\nQ1 due to marketing campaigns. Q3 had exponential growth due to global expansion.'),\n",
       " Document(metadata={'page': 0, 'type': 'image', 'image_id': 'page_0_img_0'}, page_content='[Image: page_0_img_0]')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24c017b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00267244,  0.01282998, -0.05183141, ..., -0.00385081,\n",
       "         0.02977718, -0.00010685],\n",
       "       [ 0.01732335, -0.01327693, -0.02427032, ...,  0.0899405 ,\n",
       "        -0.00272154,  0.03253041]], shape=(2, 512), dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create unified FAISS vector store with CLIP embeddings\n",
    "embeddings_array = np.array(all_embeddings)\n",
    "embeddings_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47bd688d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Document(metadata={'page': 0, 'type': 'text'}, page_content='Annual Revenue Overview\\nThis document summarizes the revenue trends across Q1, Q2, and Q3. As illustrated in the chart\\nbelow, revenue grew steadily with the highest growth recorded in Q3.\\nQ1 showed a moderate increase in revenue as new product lines were introduced. Q2 outperformed\\nQ1 due to marketing campaigns. Q3 had exponential growth due to global expansion.'),\n",
       "  Document(metadata={'page': 0, 'type': 'image', 'image_id': 'page_0_img_0'}, page_content='[Image: page_0_img_0]')],\n",
       " array([[-0.00267244,  0.01282998, -0.05183141, ..., -0.00385081,\n",
       "          0.02977718, -0.00010685],\n",
       "        [ 0.01732335, -0.01327693, -0.02427032, ...,  0.0899405 ,\n",
       "         -0.00272154,  0.03253041]], shape=(2, 512), dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(all_docs,embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de148a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x7c5325a7f170>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create custom FAISS index since we have precomputed embeddings\n",
    "embeddings_array = np.vstack(all_embeddings)  \n",
    "vector_store = FAISS.from_embeddings(\n",
    "    text_embeddings=[(doc.page_content, emb) for doc, emb in zip(all_docs, embeddings_array)],\n",
    "    embedding=None,  # We're using precomputed embeddings\n",
    "    metadatas=[doc.metadata for doc in all_docs]\n",
    ")\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ff1eeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üÜï NEW: Ollama Client Class\n",
    "class OllamaLLaVA:\n",
    "    def __init__(self, model=\"llava:7b\", base_url=\"http://localhost:11434\"):\n",
    "        self.model = model\n",
    "        self.base_url = base_url\n",
    "        \n",
    "    def generate_with_vision(self, prompt, images=None):\n",
    "        \"\"\"Generate response using Ollama's LLaVA model with vision capabilities.\"\"\"\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": 0.1,\n",
    "                \"top_p\": 0.9,\n",
    "                \"num_predict\": 1000\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add images if provided (base64 format)\n",
    "        if images:\n",
    "            payload[\"images\"] = images\n",
    "        \n",
    "        try:\n",
    "            print(f\"ü§ñ Querying {self.model}...\")\n",
    "            response = requests.post(url, json=payload, timeout=180)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            return result.get(\"response\", \"No response generated\")\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return f\"‚ùå Ollama Error: {e}\"\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Unexpected Error: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aead918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GPT-4 Vision model\n",
    "llm = OllamaLLaVA(model=\"llava:7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "366ea313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_multimodal(query, k=5):\n",
    "    \"\"\"Unified retrieval using CLIP embeddings for both text and images.\"\"\"\n",
    "    # Embed query using CLIP\n",
    "    query_embedding = embed_text(query)\n",
    "    \n",
    "    # Search in unified vector store\n",
    "    results = vector_store.similarity_search_by_vector(\n",
    "        embedding=query_embedding,\n",
    "        k=k\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b73a041",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üÜï NEW: Updated message creation for Ollama LLaVA\n",
    "def create_ollama_multimodal_message(query, retrieved_docs):\n",
    "    \"\"\"Create a message with both text and images for Ollama LLaVA.\"\"\"\n",
    "    \n",
    "    # Separate text and image documents\n",
    "    text_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"text\"]\n",
    "    image_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"image\"]\n",
    "    \n",
    "    # Build text context\n",
    "    context_parts = []\n",
    "    \n",
    "    # Add text context\n",
    "    if text_docs:\n",
    "        text_context = \"\\n\\n\".join([\n",
    "            f\"üìÑ [Page {doc.metadata['page']}]: {doc.page_content}\"\n",
    "            for doc in text_docs\n",
    "        ])\n",
    "        context_parts.append(f\"üìù **TEXT CONTENT:**\\n{text_context}\")\n",
    "    \n",
    "    # Collect images for Ollama\n",
    "    images_to_analyze = []\n",
    "    image_descriptions = []\n",
    "    \n",
    "    for doc in image_docs:\n",
    "        image_id = doc.metadata.get(\"image_id\")\n",
    "        page = doc.metadata.get('page', '?')\n",
    "        \n",
    "        if image_id and image_id in image_data_store:\n",
    "            # Add base64 image for Ollama\n",
    "            images_to_analyze.append(image_data_store[image_id])\n",
    "            image_descriptions.append(f\"üñºÔ∏è Image from page {page} (analyzing below)\")\n",
    "    \n",
    "    if image_descriptions:\n",
    "        context_parts.append(f\"üìä **VISUAL CONTENT:**\\n\" + \"\\n\".join(image_descriptions))\n",
    "    \n",
    "    # Create comprehensive prompt for LLaVA\n",
    "    full_context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    prompt = f\"\"\"üîç **MULTIMODAL PDF ANALYSIS**\n",
    "\n",
    "‚ùì **QUESTION:** {query}\n",
    "\n",
    "üìö **DOCUMENT CONTEXT:**\n",
    "{full_context}\n",
    "\n",
    "üéØ **INSTRUCTIONS:**\n",
    "1. Carefully read and understand the text content from the PDF\n",
    "2. Analyze any images/charts/diagrams provided below\n",
    "3. Provide a comprehensive answer combining insights from BOTH text and visual elements\n",
    "4. Reference specific page numbers when mentioning information\n",
    "5. If you see charts, graphs, or tables in images, describe what data they show\n",
    "6. Be specific and detailed in your analysis\n",
    "\n",
    "üí° **Your comprehensive analysis:**\"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"images\": images_to_analyze\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97a27ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîÑ UPDATED: Main pipeline function for Ollama\n",
    "def multimodal_pdf_rag_pipeline(query):\n",
    "    \"\"\"Main pipeline for multimodal RAG using Ollama LLaVA.\"\"\"\n",
    "    \n",
    "    # Retrieve relevant documents (SAME AS BEFORE ‚úÖ)\n",
    "    context_docs = retrieve_multimodal(query, k=5)\n",
    "    \n",
    "    # Create Ollama-compatible message (NEW!)\n",
    "    message_data = create_ollama_multimodal_message(query, context_docs)\n",
    "    \n",
    "    # Get response from Ollama LLaVA (NEW!)\n",
    "    response = llm.generate_with_vision(\n",
    "        prompt=message_data[\"prompt\"],\n",
    "        images=message_data[\"images\"]\n",
    "    )\n",
    "    \n",
    "    # Print retrieved context info (SAME AS BEFORE ‚úÖ)\n",
    "    print(f\"\\nüìã Retrieved {len(context_docs)} documents:\")\n",
    "    for doc in context_docs:\n",
    "        doc_type = doc.metadata.get(\"type\", \"unknown\")\n",
    "        page = doc.metadata.get(\"page\", \"?\")\n",
    "        if doc_type == \"text\":\n",
    "            preview = doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
    "            print(f\"  - üìù Text from page {page}: {preview}\")\n",
    "        else:\n",
    "            print(f\"  - üñºÔ∏è Image from page {page}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c352f248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ollama is running! Available models: ['llava:7b']\n",
      "üéâ LLaVA:7b is ready!\n",
      "\n",
      "üî• **Query:** What does the chart on page 1 show about revenue trends?\n",
      "--------------------------------------------------\n",
      "ü§ñ Querying llava:7b...\n",
      "\n",
      "üìã Retrieved 2 documents:\n",
      "  - üìù Text from page 0: Annual Revenue Overview\n",
      "This document summarizes the revenue trends across Q1, Q2, and Q3. As illust...\n",
      "  - üñºÔ∏è Image from page 0\n",
      "\n",
      "\n",
      "**Answer:** ‚ùå Ollama Error: 500 Server Error: Internal Server Error for url: http://localhost:11434/api/generate\n",
      "======================================================================\n",
      "\n",
      "üî• **Query:** Summarize the main findings from the document including visual data\n",
      "--------------------------------------------------\n",
      "ü§ñ Querying llava:7b...\n",
      "\n",
      "üìã Retrieved 2 documents:\n",
      "  - üìù Text from page 0: Annual Revenue Overview\n",
      "This document summarizes the revenue trends across Q1, Q2, and Q3. As illust...\n",
      "  - üñºÔ∏è Image from page 0\n",
      "\n",
      "\n",
      "**Answer:** ‚ùå Ollama Error: 500 Server Error: Internal Server Error for url: http://localhost:11434/api/generate\n",
      "======================================================================\n",
      "\n",
      "üî• **Query:** What visual elements are present in the document and what do they tell us?\n",
      "--------------------------------------------------\n",
      "ü§ñ Querying llava:7b...\n",
      "\n",
      "üìã Retrieved 2 documents:\n",
      "  - üìù Text from page 0: Annual Revenue Overview\n",
      "This document summarizes the revenue trends across Q1, Q2, and Q3. As illust...\n",
      "  - üñºÔ∏è Image from page 0\n",
      "\n",
      "\n",
      "**Answer:** ‚ùå Ollama Error: 500 Server Error: Internal Server Error for url: http://localhost:11434/api/generate\n",
      "======================================================================\n",
      "\n",
      "üî• **Query:** Analyze any graphs, charts, or diagrams and explain the key insights\n",
      "--------------------------------------------------\n",
      "ü§ñ Querying llava:7b...\n",
      "\n",
      "üìã Retrieved 2 documents:\n",
      "  - üìù Text from page 0: Annual Revenue Overview\n",
      "This document summarizes the revenue trends across Q1, Q2, and Q3. As illust...\n",
      "  - üñºÔ∏è Image from page 0\n",
      "\n",
      "\n",
      "**Answer:** ‚ùå Ollama Error: 500 Server Error: Internal Server Error for url: http://localhost:11434/api/generate\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# üß™ Testing with enhanced queries\n",
    "if __name__ == \"__main__\":\n",
    "    # üîß First, check if Ollama is running\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "        models = [m[\"name\"] for m in response.json()[\"models\"]]\n",
    "        print(f\"‚úÖ Ollama is running! Available models: {models}\")\n",
    "        \n",
    "        if \"llava:7b\" not in models:\n",
    "            print(\"‚ùå LLaVA:7b not found! Run: ollama pull llava:7b\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            print(\"üéâ LLaVA:7b is ready!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Ollama not running! Start with: ollama serve\\nError: {e}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Example queries optimized for vision capabilities\n",
    "    queries = [\n",
    "        \"What does the chart on page 1 show about revenue trends?\",\n",
    "        \"Summarize the main findings from the document including visual data\",\n",
    "        \"What visual elements are present in the document and what do they tell us?\",\n",
    "        \"Analyze any graphs, charts, or diagrams and explain the key insights\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nüî• **Query:** {query}\")\n",
    "        print(\"-\" * 50)\n",
    "        answer = multimodal_pdf_rag_pipeline(query)\n",
    "        print(f\"**Answer:** {answer}\")\n",
    "        print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007361d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
